{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5APfYhZwcSin",
        "outputId": "99754cd8-08e5-4fd7-8283-d873e9479f11"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d:\\\\Learn_space\\\\KhaiKhoang'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_ydm1l7asQt",
        "outputId": "56a57f2c-53af-4e94-948b-29637fd66020"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Cloning into 'VNExpressCrawler'...\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/egliette/VNExpressCrawler.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iYu7-k6jbiYD",
        "outputId": "fa580b12-a197-42a9-ad8d-11fbceeb71ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d:\\\\Learn_space\\\\KhaiKhoang\\\\VNExpressCrawler'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDawhU4oRR9p",
        "outputId": "5bc8eedf-7de8-496e-a92c-bb808df2444e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "d:\\Learn_space\\KhaiKhoang\n"
          ]
        }
      ],
      "source": [
        "cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uryF19Qze-ML",
        "outputId": "fff84522-2377-4711-cc33-49273a9a4fdf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawl articles type the-thao\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            " 33%|███▎      | 1/3 [00:00<00:01,  1.43it/s]\n",
            " 67%|██████▋   | 2/3 [00:00<00:00,  2.59it/s]\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.71it/s]\n",
            "100%|██████████| 3/3 [00:01<00:00,  1.78it/s]\n",
            "\n",
            "  0%|          | 0/106 [00:00<?, ?it/s]\n",
            "  1%|          | 1/106 [00:00<00:55,  1.90it/s]\n",
            "  2%|▏         | 2/106 [00:01<01:44,  1.00s/it]\n",
            "  3%|▎         | 3/106 [00:02<01:25,  1.20it/s]\n",
            "  4%|▍         | 4/106 [00:03<01:23,  1.22it/s]\n",
            "  5%|▍         | 5/106 [00:03<01:18,  1.29it/s]\n",
            "  6%|▌         | 6/106 [00:04<01:16,  1.31it/s]\n",
            "  7%|▋         | 7/106 [00:05<01:13,  1.35it/s]\n",
            "  8%|▊         | 8/106 [00:05<01:04,  1.52it/s]\n",
            "  8%|▊         | 9/106 [00:06<01:11,  1.35it/s]\n",
            "  9%|▉         | 10/106 [00:07<01:09,  1.38it/s]\n",
            " 10%|█         | 11/106 [00:08<01:07,  1.40it/s]\n",
            " 11%|█▏        | 12/106 [00:09<01:20,  1.17it/s]\n",
            " 12%|█▏        | 13/106 [00:09<01:12,  1.28it/s]\n",
            " 13%|█▎        | 14/106 [00:10<01:16,  1.20it/s]\n",
            " 14%|█▍        | 15/106 [00:11<01:10,  1.28it/s]\n",
            " 15%|█▌        | 16/106 [00:12<01:21,  1.10it/s]\n",
            " 16%|█▌        | 17/106 [00:13<01:20,  1.10it/s]\n",
            " 17%|█▋        | 18/106 [00:14<01:12,  1.22it/s]\n",
            " 18%|█▊        | 19/106 [00:15<01:09,  1.25it/s]\n",
            " 19%|█▉        | 20/106 [00:15<01:06,  1.30it/s]\n",
            " 20%|█▉        | 21/106 [00:16<01:04,  1.32it/s]\n",
            " 21%|██        | 22/106 [00:17<01:00,  1.40it/s]\n",
            " 22%|██▏       | 23/106 [00:17<00:52,  1.57it/s]\n",
            " 23%|██▎       | 24/106 [00:19<01:21,  1.00it/s]\n",
            " 24%|██▎       | 25/106 [00:20<01:11,  1.14it/s]\n",
            " 25%|██▍       | 26/106 [00:20<01:00,  1.32it/s]\n",
            " 25%|██▌       | 27/106 [00:21<00:55,  1.43it/s]\n",
            " 26%|██▋       | 28/106 [00:21<00:52,  1.49it/s]\n",
            " 27%|██▋       | 29/106 [00:22<00:52,  1.46it/s]\n",
            " 28%|██▊       | 30/106 [00:23<00:56,  1.36it/s]\n",
            " 29%|██▉       | 31/106 [00:23<00:53,  1.40it/s]\n",
            " 30%|███       | 32/106 [00:24<00:51,  1.44it/s]\n",
            " 31%|███       | 33/106 [00:25<00:49,  1.47it/s]\n",
            " 32%|███▏      | 34/106 [00:25<00:51,  1.40it/s]\n",
            " 33%|███▎      | 35/106 [00:27<01:07,  1.06it/s]\n",
            " 34%|███▍      | 36/106 [00:27<00:56,  1.23it/s]\n",
            " 35%|███▍      | 37/106 [00:28<00:52,  1.31it/s]\n",
            " 36%|███▌      | 38/106 [00:29<00:49,  1.37it/s]\n",
            " 37%|███▋      | 39/106 [00:30<00:50,  1.32it/s]\n",
            " 38%|███▊      | 40/106 [00:30<00:52,  1.26it/s]\n",
            " 39%|███▊      | 41/106 [00:31<00:49,  1.32it/s]\n",
            " 40%|███▉      | 42/106 [00:32<00:43,  1.46it/s]\n",
            " 41%|████      | 43/106 [00:33<00:46,  1.34it/s]\n",
            " 42%|████▏     | 44/106 [00:33<00:44,  1.38it/s]\n",
            " 42%|████▏     | 45/106 [00:34<00:41,  1.45it/s]\n",
            " 43%|████▎     | 46/106 [00:35<00:41,  1.44it/s]\n",
            " 44%|████▍     | 47/106 [00:36<00:46,  1.26it/s]\n",
            " 45%|████▌     | 48/106 [00:36<00:42,  1.37it/s]\n",
            " 46%|████▌     | 49/106 [00:37<00:40,  1.40it/s]\n",
            " 47%|████▋     | 50/106 [00:37<00:38,  1.45it/s]\n",
            " 48%|████▊     | 51/106 [00:38<00:35,  1.54it/s]\n",
            " 49%|████▉     | 52/106 [00:39<00:41,  1.29it/s]\n",
            " 50%|█████     | 53/106 [00:40<00:37,  1.42it/s]\n",
            " 51%|█████     | 54/106 [00:40<00:34,  1.50it/s]\n",
            " 52%|█████▏    | 55/106 [00:41<00:33,  1.53it/s]\n",
            " 53%|█████▎    | 56/106 [00:42<00:35,  1.42it/s]\n",
            " 54%|█████▍    | 57/106 [00:42<00:34,  1.43it/s]\n",
            " 55%|█████▍    | 58/106 [00:44<00:42,  1.12it/s]\n",
            " 56%|█████▌    | 59/106 [00:45<00:46,  1.01it/s]\n",
            " 57%|█████▋    | 60/106 [00:46<00:40,  1.13it/s]\n",
            " 58%|█████▊    | 61/106 [00:46<00:30,  1.47it/s]\n",
            " 58%|█████▊    | 62/106 [00:46<00:23,  1.84it/s]\n",
            " 59%|█████▉    | 63/106 [00:47<00:24,  1.78it/s]\n",
            " 60%|██████    | 64/106 [00:48<00:28,  1.46it/s]\n",
            " 61%|██████▏   | 65/106 [00:48<00:26,  1.55it/s]\n",
            " 62%|██████▏   | 66/106 [00:49<00:26,  1.50it/s]\n",
            " 63%|██████▎   | 67/106 [00:49<00:25,  1.52it/s]\n",
            " 64%|██████▍   | 68/106 [00:50<00:24,  1.53it/s]\n",
            " 65%|██████▌   | 69/106 [00:51<00:23,  1.58it/s]\n",
            " 66%|██████▌   | 70/106 [00:51<00:23,  1.50it/s]\n",
            " 67%|██████▋   | 71/106 [00:52<00:23,  1.51it/s]\n",
            " 68%|██████▊   | 72/106 [00:53<00:21,  1.59it/s]\n",
            " 69%|██████▉   | 73/106 [00:53<00:22,  1.46it/s]\n",
            " 70%|██████▉   | 74/106 [00:54<00:24,  1.31it/s]\n",
            " 71%|███████   | 75/106 [00:55<00:24,  1.27it/s]\n",
            " 72%|███████▏  | 76/106 [00:56<00:22,  1.32it/s]\n",
            " 73%|███████▎  | 77/106 [00:58<00:30,  1.07s/it]\n",
            " 74%|███████▎  | 78/106 [00:58<00:26,  1.04it/s]\n",
            " 75%|███████▍  | 79/106 [00:59<00:23,  1.17it/s]\n",
            " 75%|███████▌  | 80/106 [01:00<00:20,  1.28it/s]\n",
            " 76%|███████▋  | 81/106 [01:01<00:20,  1.23it/s]\n",
            " 77%|███████▋  | 82/106 [01:03<00:31,  1.30s/it]\n",
            " 78%|███████▊  | 83/106 [01:04<00:25,  1.13s/it]\n",
            " 79%|███████▉  | 84/106 [01:06<00:30,  1.37s/it]\n",
            " 80%|████████  | 85/106 [01:08<00:33,  1.61s/it]\n",
            " 81%|████████  | 86/106 [01:08<00:26,  1.32s/it]\n",
            " 82%|████████▏ | 87/106 [01:09<00:21,  1.14s/it]\n",
            " 83%|████████▎ | 88/106 [01:10<00:17,  1.01it/s]\n",
            " 84%|████████▍ | 89/106 [01:11<00:16,  1.06it/s]\n",
            " 85%|████████▍ | 90/106 [01:11<00:13,  1.16it/s]\n",
            " 86%|████████▌ | 91/106 [01:12<00:11,  1.29it/s]\n",
            " 87%|████████▋ | 92/106 [01:12<00:10,  1.38it/s]\n",
            " 88%|████████▊ | 93/106 [01:14<00:12,  1.08it/s]\n",
            " 89%|████████▊ | 94/106 [01:15<00:10,  1.18it/s]\n",
            " 90%|████████▉ | 95/106 [01:16<00:12,  1.14s/it]\n",
            " 91%|█████████ | 96/106 [01:18<00:12,  1.26s/it]\n",
            " 92%|█████████▏| 97/106 [01:19<00:09,  1.11s/it]\n",
            " 92%|█████████▏| 98/106 [01:19<00:08,  1.01s/it]\n",
            " 93%|█████████▎| 99/106 [01:21<00:08,  1.21s/it]\n",
            " 94%|█████████▍| 100/106 [01:22<00:06,  1.02s/it]\n",
            " 95%|█████████▌| 101/106 [01:22<00:04,  1.07it/s]\n",
            " 96%|█████████▌| 102/106 [01:23<00:03,  1.17it/s]\n",
            " 97%|█████████▋| 103/106 [01:24<00:02,  1.06it/s]\n",
            " 98%|█████████▊| 104/106 [01:26<00:02,  1.07s/it]\n",
            " 99%|█████████▉| 105/106 [01:26<00:00,  1.10it/s]\n",
            "100%|██████████| 106/106 [01:26<00:00,  1.43it/s]\n",
            "100%|██████████| 106/106 [01:26<00:00,  1.22it/s]\n"
          ]
        }
      ],
      "source": [
        "!python ./VNExpressCrawler/types_crawler.py --type the-thao --pages 3 --output ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlDryT97gw0G",
        "outputId": "fbc96bc4-e1bd-449e-a887-be2b267984b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawl articles type doi-song\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            " 33%|███▎      | 1/3 [00:00<00:01,  1.89it/s]\n",
            " 67%|██████▋   | 2/3 [00:01<00:00,  1.61it/s]\n",
            "100%|██████████| 3/3 [00:01<00:00,  2.41it/s]\n",
            "100%|██████████| 3/3 [00:01<00:00,  2.17it/s]\n",
            "\n",
            "  0%|          | 0/104 [00:00<?, ?it/s]\n",
            "  1%|          | 1/104 [00:00<01:34,  1.09it/s]\n",
            "  2%|▏         | 2/104 [00:01<01:14,  1.36it/s]\n",
            "  3%|▎         | 3/104 [00:02<01:09,  1.45it/s]\n",
            "  4%|▍         | 4/104 [00:02<01:12,  1.38it/s]\n",
            "  5%|▍         | 5/104 [00:03<01:12,  1.36it/s]\n",
            "  6%|▌         | 6/104 [00:04<01:10,  1.40it/s]\n",
            "  7%|▋         | 7/104 [00:04<01:06,  1.47it/s]\n",
            "  8%|▊         | 8/104 [00:05<01:00,  1.58it/s]\n",
            "  9%|▊         | 9/104 [00:06<00:59,  1.61it/s]\n",
            " 10%|▉         | 10/104 [00:06<00:56,  1.66it/s]\n",
            " 11%|█         | 11/104 [00:07<01:14,  1.26it/s]\n",
            " 12%|█▏        | 12/104 [00:08<01:14,  1.23it/s]\n",
            " 12%|█▎        | 13/104 [00:09<01:07,  1.35it/s]\n",
            " 13%|█▎        | 14/104 [00:09<01:03,  1.42it/s]\n",
            " 14%|█▍        | 15/104 [00:10<00:58,  1.52it/s]\n",
            " 15%|█▌        | 16/104 [00:11<01:13,  1.20it/s]\n",
            " 16%|█▋        | 17/104 [00:12<01:11,  1.22it/s]\n",
            " 17%|█▋        | 18/104 [00:13<01:05,  1.30it/s]\n",
            " 18%|█▊        | 19/104 [00:13<01:00,  1.40it/s]\n",
            " 19%|█▉        | 20/104 [00:14<00:54,  1.54it/s]\n",
            " 20%|██        | 21/104 [00:14<00:44,  1.88it/s]\n",
            " 21%|██        | 22/104 [00:15<00:50,  1.62it/s]\n",
            " 22%|██▏       | 23/104 [00:15<00:49,  1.62it/s]\n",
            " 23%|██▎       | 24/104 [00:16<00:52,  1.53it/s]\n",
            " 24%|██▍       | 25/104 [00:17<00:48,  1.62it/s]\n",
            " 25%|██▌       | 26/104 [00:17<00:47,  1.64it/s]\n",
            " 26%|██▌       | 27/104 [00:18<00:57,  1.34it/s]\n",
            " 27%|██▋       | 28/104 [00:19<00:52,  1.45it/s]\n",
            " 28%|██▊       | 29/104 [00:19<00:48,  1.53it/s]\n",
            " 29%|██▉       | 30/104 [00:20<00:45,  1.61it/s]\n",
            " 30%|██▉       | 31/104 [00:21<00:44,  1.63it/s]\n",
            " 31%|███       | 32/104 [00:21<00:42,  1.68it/s]\n",
            " 32%|███▏      | 33/104 [00:23<00:57,  1.23it/s]\n",
            " 33%|███▎      | 34/104 [00:23<00:48,  1.44it/s]\n",
            " 34%|███▎      | 35/104 [00:24<00:45,  1.50it/s]\n",
            " 35%|███▍      | 36/104 [00:24<00:43,  1.56it/s]\n",
            " 36%|███▌      | 37/104 [00:25<00:41,  1.63it/s]\n",
            " 37%|███▋      | 38/104 [00:25<00:39,  1.67it/s]\n",
            " 38%|███▊      | 39/104 [00:26<00:37,  1.75it/s]\n",
            " 38%|███▊      | 40/104 [00:26<00:38,  1.68it/s]\n",
            " 39%|███▉      | 41/104 [00:27<00:38,  1.64it/s]\n",
            " 40%|████      | 42/104 [00:28<00:36,  1.72it/s]\n",
            " 41%|████▏     | 43/104 [00:28<00:38,  1.58it/s]\n",
            " 42%|████▏     | 44/104 [00:29<00:39,  1.53it/s]\n",
            " 43%|████▎     | 45/104 [00:30<00:39,  1.49it/s]\n",
            " 44%|████▍     | 46/104 [00:30<00:38,  1.53it/s]\n",
            " 45%|████▌     | 47/104 [00:31<00:35,  1.62it/s]\n",
            " 46%|████▌     | 48/104 [00:32<00:40,  1.39it/s]\n",
            " 47%|████▋     | 49/104 [00:33<00:55,  1.00s/it]\n",
            " 48%|████▊     | 50/104 [00:35<01:01,  1.14s/it]\n",
            " 49%|████▉     | 51/104 [00:37<01:13,  1.39s/it]\n",
            " 50%|█████     | 52/104 [00:37<00:59,  1.14s/it]\n",
            " 51%|█████     | 53/104 [00:39<01:11,  1.41s/it]\n",
            " 52%|█████▏    | 54/104 [00:41<01:13,  1.46s/it]\n",
            " 53%|█████▎    | 55/104 [00:42<00:57,  1.18s/it]\n",
            " 54%|█████▍    | 56/104 [00:43<00:59,  1.24s/it]\n",
            " 55%|█████▍    | 57/104 [00:43<00:47,  1.01s/it]\n",
            " 56%|█████▌    | 58/104 [00:45<00:52,  1.14s/it]\n",
            " 57%|█████▋    | 59/104 [00:45<00:42,  1.06it/s]\n",
            " 58%|█████▊    | 60/104 [00:46<00:38,  1.13it/s]\n",
            " 59%|█████▊    | 61/104 [00:47<00:34,  1.26it/s]\n",
            " 60%|█████▉    | 62/104 [00:47<00:30,  1.39it/s]\n",
            " 61%|██████    | 63/104 [00:49<00:37,  1.09it/s]\n",
            " 62%|██████▏   | 64/104 [00:49<00:31,  1.29it/s]\n",
            " 62%|██████▎   | 65/104 [00:50<00:28,  1.35it/s]\n",
            " 63%|██████▎   | 66/104 [00:51<00:34,  1.11it/s]\n",
            " 64%|██████▍   | 67/104 [00:52<00:31,  1.18it/s]\n",
            " 65%|██████▌   | 68/104 [00:52<00:27,  1.29it/s]\n",
            " 66%|██████▋   | 69/104 [00:53<00:24,  1.44it/s]\n",
            " 67%|██████▋   | 70/104 [00:53<00:22,  1.51it/s]\n",
            " 68%|██████▊   | 71/104 [00:55<00:35,  1.08s/it]\n",
            " 69%|██████▉   | 72/104 [00:57<00:34,  1.07s/it]\n",
            " 70%|███████   | 73/104 [00:57<00:28,  1.08it/s]\n",
            " 71%|███████   | 74/104 [00:58<00:24,  1.23it/s]\n",
            " 72%|███████▏  | 75/104 [00:58<00:21,  1.34it/s]\n",
            " 73%|███████▎  | 76/104 [00:59<00:21,  1.31it/s]\n",
            " 74%|███████▍  | 77/104 [01:00<00:19,  1.38it/s]\n",
            " 75%|███████▌  | 78/104 [01:02<00:27,  1.07s/it]\n",
            " 76%|███████▌  | 79/104 [01:03<00:27,  1.10s/it]\n",
            " 77%|███████▋  | 80/104 [01:04<00:29,  1.23s/it]\n",
            " 78%|███████▊  | 81/104 [01:05<00:23,  1.03s/it]\n",
            " 79%|███████▉  | 82/104 [01:05<00:19,  1.10it/s]\n",
            " 80%|███████▉  | 83/104 [01:06<00:18,  1.12it/s]\n",
            " 81%|████████  | 84/104 [01:07<00:15,  1.29it/s]\n",
            " 82%|████████▏ | 85/104 [01:07<00:13,  1.42it/s]\n",
            " 83%|████████▎ | 86/104 [01:09<00:15,  1.13it/s]\n",
            " 84%|████████▎ | 87/104 [01:09<00:14,  1.17it/s]\n",
            " 85%|████████▍ | 88/104 [01:11<00:16,  1.06s/it]\n",
            " 86%|████████▌ | 89/104 [01:12<00:14,  1.04it/s]\n",
            " 87%|████████▋ | 90/104 [01:14<00:17,  1.24s/it]\n",
            " 88%|████████▊ | 91/104 [01:14<00:13,  1.06s/it]\n",
            " 88%|████████▊ | 92/104 [01:16<00:13,  1.15s/it]\n",
            " 89%|████████▉ | 93/104 [01:16<00:10,  1.03it/s]\n",
            " 90%|█████████ | 94/104 [01:18<00:11,  1.14s/it]\n",
            " 91%|█████████▏| 95/104 [01:18<00:08,  1.04it/s]\n",
            " 92%|█████████▏| 96/104 [01:21<00:11,  1.39s/it]\n",
            " 93%|█████████▎| 97/104 [01:23<00:11,  1.64s/it]\n",
            " 94%|█████████▍| 98/104 [01:24<00:08,  1.49s/it]\n",
            " 95%|█████████▌| 99/104 [01:25<00:07,  1.45s/it]\n",
            " 96%|█████████▌| 100/104 [01:26<00:04,  1.22s/it]\n",
            " 97%|█████████▋| 101/104 [01:27<00:03,  1.03s/it]\n",
            " 98%|█████████▊| 102/104 [01:27<00:01,  1.07it/s]\n",
            " 99%|█████████▉| 103/104 [01:29<00:01,  1.13s/it]\n",
            "100%|██████████| 104/104 [01:29<00:00,  1.20it/s]\n",
            "100%|██████████| 104/104 [01:29<00:00,  1.16it/s]\n"
          ]
        }
      ],
      "source": [
        "!python ./VNExpressCrawler/types_crawler.py --type doi-song --pages 3 --output ./data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KE95KIoTgw5P",
        "outputId": "ec205148-4a94-418e-bd96-d4c7d20b38ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawl articles type du-lich\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/3 [00:00<?, ?it/s]\n",
            " 33%|███▎      | 1/3 [00:00<00:00,  2.21it/s]\n",
            " 67%|██████▋   | 2/3 [00:00<00:00,  3.48it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00,  4.39it/s]\n",
            "100%|██████████| 3/3 [00:00<00:00,  3.84it/s]\n",
            "\n",
            "  0%|          | 0/121 [00:00<?, ?it/s]\n",
            "  1%|          | 1/121 [00:00<01:07,  1.78it/s]\n",
            "  2%|▏         | 2/121 [00:01<01:08,  1.74it/s]\n",
            "  2%|▏         | 3/121 [00:01<01:15,  1.56it/s]\n",
            "  3%|▎         | 4/121 [00:02<01:25,  1.36it/s]\n",
            "  4%|▍         | 5/121 [00:03<01:15,  1.54it/s]\n",
            "  5%|▍         | 6/121 [00:03<00:58,  1.97it/s]\n",
            "  6%|▌         | 7/121 [00:04<01:03,  1.80it/s]\n",
            "  7%|▋         | 8/121 [00:04<01:03,  1.79it/s]\n",
            "  7%|▋         | 9/121 [00:04<00:51,  2.16it/s]\n",
            "  8%|▊         | 10/121 [00:05<01:01,  1.82it/s]\n",
            "  9%|▉         | 11/121 [00:06<01:00,  1.81it/s]\n",
            " 10%|▉         | 12/121 [00:07<01:17,  1.40it/s]\n",
            " 11%|█         | 13/121 [00:09<01:57,  1.08s/it]\n",
            " 12%|█▏        | 14/121 [00:10<01:48,  1.01s/it]\n",
            " 12%|█▏        | 15/121 [00:11<01:56,  1.10s/it]\n",
            " 13%|█▎        | 16/121 [00:12<01:41,  1.04it/s]\n",
            " 14%|█▍        | 17/121 [00:14<02:10,  1.26s/it]\n",
            " 15%|█▍        | 18/121 [00:14<01:54,  1.11s/it]\n",
            " 16%|█▌        | 19/121 [00:15<01:32,  1.10it/s]\n",
            " 17%|█▋        | 20/121 [00:15<01:22,  1.23it/s]\n",
            " 17%|█▋        | 21/121 [00:16<01:15,  1.33it/s]\n",
            " 18%|█▊        | 22/121 [00:17<01:11,  1.38it/s]\n",
            " 19%|█▉        | 23/121 [00:17<01:09,  1.41it/s]\n",
            " 20%|█▉        | 24/121 [00:18<01:02,  1.54it/s]\n",
            " 21%|██        | 25/121 [00:19<01:08,  1.40it/s]\n",
            " 21%|██▏       | 26/121 [00:19<01:05,  1.45it/s]\n",
            " 22%|██▏       | 27/121 [00:20<01:02,  1.51it/s]\n",
            " 23%|██▎       | 28/121 [00:20<00:58,  1.58it/s]\n",
            " 24%|██▍       | 29/121 [00:21<01:02,  1.46it/s]\n",
            " 25%|██▍       | 30/121 [00:22<01:00,  1.50it/s]\n",
            " 26%|██▌       | 31/121 [00:22<00:57,  1.58it/s]\n",
            " 26%|██▋       | 32/121 [00:23<01:00,  1.47it/s]\n",
            " 27%|██▋       | 33/121 [00:24<00:56,  1.55it/s]\n",
            " 28%|██▊       | 34/121 [00:26<01:25,  1.01it/s]\n",
            " 29%|██▉       | 35/121 [00:26<01:15,  1.14it/s]\n",
            " 30%|██▉       | 36/121 [00:27<01:25,  1.01s/it]\n",
            " 31%|███       | 37/121 [00:29<01:36,  1.15s/it]\n",
            " 31%|███▏      | 38/121 [00:29<01:17,  1.07it/s]\n",
            " 32%|███▏      | 39/121 [00:30<01:05,  1.25it/s]\n",
            " 33%|███▎      | 40/121 [00:31<01:02,  1.29it/s]\n",
            " 34%|███▍      | 41/121 [00:31<01:01,  1.31it/s]\n",
            " 35%|███▍      | 42/121 [00:32<00:54,  1.46it/s]\n",
            " 36%|███▌      | 43/121 [00:32<00:49,  1.59it/s]\n",
            " 36%|███▋      | 44/121 [00:33<00:44,  1.74it/s]\n",
            " 37%|███▋      | 45/121 [00:33<00:41,  1.83it/s]\n",
            " 38%|███▊      | 46/121 [00:34<00:38,  1.97it/s]\n",
            " 39%|███▉      | 47/121 [00:35<00:51,  1.45it/s]\n",
            " 40%|███▉      | 48/121 [00:35<00:44,  1.66it/s]\n",
            " 40%|████      | 49/121 [00:36<00:45,  1.59it/s]\n",
            " 41%|████▏     | 50/121 [00:36<00:41,  1.71it/s]\n",
            " 42%|████▏     | 51/121 [00:37<00:41,  1.67it/s]\n",
            " 43%|████▎     | 52/121 [00:38<00:39,  1.76it/s]\n",
            " 44%|████▍     | 53/121 [00:38<00:36,  1.86it/s]\n",
            " 45%|████▍     | 54/121 [00:39<00:37,  1.76it/s]\n",
            " 45%|████▌     | 55/121 [00:39<00:36,  1.83it/s]\n",
            " 46%|████▋     | 56/121 [00:40<00:37,  1.73it/s]\n",
            " 47%|████▋     | 57/121 [00:40<00:36,  1.74it/s]\n",
            " 48%|████▊     | 58/121 [00:41<00:36,  1.75it/s]\n",
            " 49%|████▉     | 59/121 [00:41<00:33,  1.83it/s]\n",
            " 50%|████▉     | 60/121 [00:42<00:33,  1.83it/s]\n",
            " 50%|█████     | 61/121 [00:43<00:33,  1.78it/s]\n",
            " 51%|█████     | 62/121 [00:43<00:33,  1.74it/s]\n",
            " 52%|█████▏    | 63/121 [00:44<00:42,  1.37it/s]\n",
            " 53%|█████▎    | 64/121 [00:46<00:55,  1.03it/s]\n",
            " 54%|█████▎    | 65/121 [00:46<00:47,  1.17it/s]\n",
            " 55%|█████▍    | 66/121 [00:47<00:42,  1.29it/s]\n",
            " 55%|█████▌    | 67/121 [00:48<00:51,  1.06it/s]\n",
            " 56%|█████▌    | 68/121 [00:49<00:44,  1.19it/s]\n",
            " 57%|█████▋    | 69/121 [00:49<00:40,  1.28it/s]\n",
            " 58%|█████▊    | 70/121 [00:50<00:37,  1.36it/s]\n",
            " 59%|█████▊    | 71/121 [00:51<00:36,  1.39it/s]\n",
            " 60%|█████▉    | 72/121 [00:53<00:50,  1.03s/it]\n",
            " 60%|██████    | 73/121 [00:53<00:42,  1.12it/s]\n",
            " 61%|██████    | 74/121 [00:54<00:39,  1.19it/s]\n",
            " 62%|██████▏   | 75/121 [00:55<00:36,  1.28it/s]\n",
            " 63%|██████▎   | 76/121 [00:56<00:42,  1.05it/s]\n",
            " 64%|██████▎   | 77/121 [00:57<00:39,  1.11it/s]\n",
            " 64%|██████▍   | 78/121 [00:58<00:45,  1.06s/it]\n",
            " 65%|██████▌   | 79/121 [01:00<00:51,  1.22s/it]\n",
            " 66%|██████▌   | 80/121 [01:01<00:51,  1.25s/it]\n",
            " 67%|██████▋   | 81/121 [01:03<01:01,  1.54s/it]\n",
            " 68%|██████▊   | 82/121 [01:04<00:50,  1.28s/it]\n",
            " 69%|██████▊   | 83/121 [01:05<00:42,  1.13s/it]\n",
            " 69%|██████▉   | 84/121 [01:05<00:37,  1.01s/it]\n",
            " 70%|███████   | 85/121 [01:06<00:33,  1.07it/s]\n",
            " 71%|███████   | 86/121 [01:07<00:29,  1.20it/s]\n",
            " 72%|███████▏  | 87/121 [01:08<00:27,  1.23it/s]\n",
            " 73%|███████▎  | 88/121 [01:09<00:37,  1.13s/it]\n",
            " 74%|███████▎  | 89/121 [01:10<00:31,  1.03it/s]\n",
            " 74%|███████▍  | 90/121 [01:11<00:28,  1.11it/s]\n",
            " 75%|███████▌  | 91/121 [01:13<00:40,  1.36s/it]\n",
            " 76%|███████▌  | 92/121 [01:14<00:33,  1.17s/it]\n",
            " 77%|███████▋  | 93/121 [01:15<00:28,  1.01s/it]\n",
            " 78%|███████▊  | 94/121 [01:17<00:35,  1.32s/it]\n",
            " 79%|███████▊  | 95/121 [01:17<00:29,  1.12s/it]\n",
            " 79%|███████▉  | 96/121 [01:19<00:30,  1.20s/it]\n",
            " 80%|████████  | 97/121 [01:21<00:38,  1.60s/it]\n",
            " 81%|████████  | 98/121 [01:22<00:30,  1.31s/it]\n",
            " 82%|████████▏ | 99/121 [01:23<00:25,  1.17s/it]\n",
            " 83%|████████▎ | 100/121 [01:23<00:20,  1.04it/s]\n",
            " 83%|████████▎ | 101/121 [01:24<00:16,  1.18it/s]\n",
            " 84%|████████▍ | 102/121 [01:24<00:15,  1.25it/s]\n",
            " 85%|████████▌ | 103/121 [01:26<00:18,  1.03s/it]\n",
            " 86%|████████▌ | 104/121 [01:27<00:16,  1.06it/s]\n",
            " 87%|████████▋ | 105/121 [01:27<00:12,  1.24it/s]\n",
            " 88%|████████▊ | 106/121 [01:29<00:17,  1.17s/it]\n",
            " 88%|████████▊ | 107/121 [01:30<00:14,  1.02s/it]\n",
            " 89%|████████▉ | 108/121 [01:31<00:15,  1.20s/it]\n",
            " 90%|█████████ | 109/121 [01:32<00:12,  1.03s/it]\n",
            " 91%|█████████ | 110/121 [01:34<00:13,  1.22s/it]\n",
            " 92%|█████████▏| 111/121 [01:36<00:14,  1.43s/it]\n",
            " 93%|█████████▎| 112/121 [01:36<00:10,  1.19s/it]\n",
            " 93%|█████████▎| 113/121 [01:37<00:08,  1.03s/it]\n",
            " 94%|█████████▍| 114/121 [01:38<00:07,  1.07s/it]\n",
            " 95%|█████████▌| 115/121 [01:39<00:05,  1.12it/s]\n",
            " 96%|█████████▌| 116/121 [01:39<00:03,  1.27it/s]\n",
            " 97%|█████████▋| 117/121 [01:40<00:02,  1.34it/s]\n",
            " 98%|█████████▊| 118/121 [01:41<00:02,  1.19it/s]\n",
            " 98%|█████████▊| 119/121 [01:42<00:01,  1.12it/s]\n",
            " 99%|█████████▉| 120/121 [01:42<00:00,  1.26it/s]\n",
            "100%|██████████| 121/121 [01:43<00:00,  1.66it/s]\n",
            "100%|██████████| 121/121 [01:43<00:00,  1.17it/s]\n"
          ]
        }
      ],
      "source": [
        "!python ./VNExpressCrawler/types_crawler.py --type du-lich --pages 3 --output ./data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfQCTu4af367"
      },
      "source": [
        "##Chuẩn hóa unicode tiếng Việt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "HuOiFnKwcyM7"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "\n",
        "uniChars = \"àáảãạâầấẩẫậăằắẳẵặèéẻẽẹêềếểễệđìíỉĩịòóỏõọôồốổỗộơờớởỡợùúủũụưừứửữựỳýỷỹỵÀÁẢÃẠÂẦẤẨẪẬĂẰẮẲẴẶÈÉẺẼẸÊỀẾỂỄỆĐÌÍỈĨỊÒÓỎÕỌÔỒỐỔỖỘƠỜỚỞỠỢÙÚỦŨỤƯỪỨỬỮỰỲÝỶỸỴÂĂĐÔƠƯ\"\n",
        "unsignChars = \"aaaaaaaaaaaaaaaaaeeeeeeeeeeediiiiiooooooooooooooooouuuuuuuuuuuyyyyyAAAAAAAAAAAAAAAAAEEEEEEEEEEEDIIIOOOOOOOOOOOOOOOOOOOUUUUUUUUUUUYYYYYAADOOU\"\n",
        "\n",
        "\n",
        "def loaddicchar():\n",
        "    dic = {}\n",
        "    char1252 = 'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ'.split(\n",
        "        '|')\n",
        "    charutf8 = \"à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ\".split(\n",
        "        '|')\n",
        "    for i in range(len(char1252)):\n",
        "        dic[char1252[i]] = charutf8[i]\n",
        "    return dic\n",
        "\n",
        "\n",
        "dicchar = loaddicchar()\n",
        "\n",
        "# Đưa toàn bộ dữ liệu qua hàm này để chuẩn hóa lại\n",
        "def convert_unicode(txt):\n",
        "    return re.sub(\n",
        "        r'à|á|ả|ã|ạ|ầ|ấ|ẩ|ẫ|ậ|ằ|ắ|ẳ|ẵ|ặ|è|é|ẻ|ẽ|ẹ|ề|ế|ể|ễ|ệ|ì|í|ỉ|ĩ|ị|ò|ó|ỏ|õ|ọ|ồ|ố|ổ|ỗ|ộ|ờ|ớ|ở|ỡ|ợ|ù|ú|ủ|ũ|ụ|ừ|ứ|ử|ữ|ự|ỳ|ý|ỷ|ỹ|ỵ|À|Á|Ả|Ã|Ạ|Ầ|Ấ|Ẩ|Ẫ|Ậ|Ằ|Ắ|Ẳ|Ẵ|Ặ|È|É|Ẻ|Ẽ|Ẹ|Ề|Ế|Ể|Ễ|Ệ|Ì|Í|Ỉ|Ĩ|Ị|Ò|Ó|Ỏ|Õ|Ọ|Ồ|Ố|Ổ|Ỗ|Ộ|Ờ|Ớ|Ở|Ỡ|Ợ|Ù|Ú|Ủ|Ũ|Ụ|Ừ|Ứ|Ử|Ữ|Ự|Ỳ|Ý|Ỷ|Ỹ|Ỵ',\n",
        "        lambda x: dicchar[x.group()], txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eATcP3vKf3Tr"
      },
      "source": [
        "##Chuấn hóa kiểu gõ dấu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ToDFmDamcyPL"
      },
      "outputs": [],
      "source": [
        "nguyen_am_to_ids = {}\n",
        "def chuan_hoa_dau_tu_tieng_viet(word):\n",
        "    if not is_valid_vietnam_word(word):\n",
        "        return word\n",
        "\n",
        "    chars = list(word)\n",
        "    dau_cau = 0\n",
        "    nguyen_am_index = []\n",
        "    qu_or_gi = False\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x == -1:\n",
        "            continue\n",
        "        elif x == 9:  # check qu\n",
        "            if index != 0 and chars[index - 1] == 'q':\n",
        "                chars[index] = 'u'\n",
        "                qu_or_gi = True\n",
        "        elif x == 5:  # check gi\n",
        "            if index != 0 and chars[index - 1] == 'g':\n",
        "                chars[index] = 'i'\n",
        "                qu_or_gi = True\n",
        "        if y != 0:\n",
        "            dau_cau = y\n",
        "            chars[index] = bang_nguyen_am[x][0]\n",
        "        if not qu_or_gi or index != 1:\n",
        "            nguyen_am_index.append(index)\n",
        "    if len(nguyen_am_index) < 2:\n",
        "        if qu_or_gi:\n",
        "            if len(chars) == 2:\n",
        "                x, y = nguyen_am_to_ids.get(chars[1])\n",
        "                chars[1] = bang_nguyen_am[x][dau_cau]\n",
        "            else:\n",
        "                x, y = nguyen_am_to_ids.get(chars[2], (-1, -1))\n",
        "                if x != -1:\n",
        "                    chars[2] = bang_nguyen_am[x][dau_cau]\n",
        "                else:\n",
        "                    chars[1] = bang_nguyen_am[5][dau_cau] if chars[1] == 'i' else bang_nguyen_am[9][dau_cau]\n",
        "            return ''.join(chars)\n",
        "        return word\n",
        "\n",
        "    for index in nguyen_am_index:\n",
        "        x, y = nguyen_am_to_ids[chars[index]]\n",
        "        if x == 4 or x == 8:  # ê, ơ\n",
        "            chars[index] = bang_nguyen_am[x][dau_cau]\n",
        "            # for index2 in nguyen_am_index:\n",
        "            #     if index2 != index:\n",
        "            #         x, y = nguyen_am_to_ids[chars[index]]\n",
        "            #         chars[index2] = bang_nguyen_am[x][0]\n",
        "            return ''.join(chars)\n",
        "\n",
        "    if len(nguyen_am_index) == 2:\n",
        "        if nguyen_am_index[-1] == len(chars) - 1:\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            chars[nguyen_am_index[0]] = bang_nguyen_am[x][dau_cau]\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            # chars[nguyen_am_index[1]] = bang_nguyen_am[x][0]\n",
        "        else:\n",
        "            # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "            # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "            x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "            chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "    else:\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[0]]]\n",
        "        # chars[nguyen_am_index[0]] = bang_nguyen_am[x][0]\n",
        "        x, y = nguyen_am_to_ids[chars[nguyen_am_index[1]]]\n",
        "        chars[nguyen_am_index[1]] = bang_nguyen_am[x][dau_cau]\n",
        "        # x, y = nguyen_am_to_ids[chars[nguyen_am_index[2]]]\n",
        "        # chars[nguyen_am_index[2]] = bang_nguyen_am[x][0]\n",
        "    return ''.join(chars)\n",
        "\n",
        "\n",
        "def is_valid_vietnam_word(word):\n",
        "    chars = list(word)\n",
        "    nguyen_am_index = -1\n",
        "    for index, char in enumerate(chars):\n",
        "        x, y = nguyen_am_to_ids.get(char, (-1, -1))\n",
        "        if x != -1:\n",
        "            if nguyen_am_index == -1:\n",
        "                nguyen_am_index = index\n",
        "            else:\n",
        "                if index - nguyen_am_index != 1:\n",
        "                    return False\n",
        "                nguyen_am_index = index\n",
        "    return True\n",
        "\n",
        "\n",
        "def chuan_hoa_dau_cau_tieng_viet(sentence):\n",
        "    \"\"\"\n",
        "        Chuyển câu tiếng việt về chuẩn gõ dấu kiểu cũ.\n",
        "        :param sentence:\n",
        "        :return:\n",
        "        \"\"\"\n",
        "    sentence = sentence.lower()\n",
        "    words = sentence.split()\n",
        "    for index, word in enumerate(words):\n",
        "        words[index] = chuan_hoa_dau_tu_tieng_viet(word)\n",
        "    return ' '.join(words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ub_yYlLQkhpq"
      },
      "source": [
        "##Tách từ Tiếng Việt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YC9qCBdlFAD"
      },
      "outputs": [],
      "source": [
        "!pip install underthesea"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0mNOhV7cyRS",
        "outputId": "1df8fff2-dd7c-4435-a322-c25eeaa009a6"
      },
      "outputs": [],
      "source": [
        "from underthesea import word_tokenize\n",
        "# sentence = 'Chàng trai 9X Quảng Trị khởi nghiệp từ nấm sò'\n",
        "\n",
        "# word_tokenize(sentence)\n",
        "# word_tokenize(sentence, format=\"text\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUyGj3SVnYVJ"
      },
      "source": [
        "## Loại bỏ stopword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pugbPEHrcyV1"
      },
      "outputs": [],
      "source": [
        "# Danh sách stopword\n",
        "stopword = [\"và\",\"của\",\"là\",\"có\" ,\"được\",\"trong\",\"một\",\"cho\",\"với\",\"không\",\"các\",\"người\",\"khi\",\"này\",\"đến\",\"để\",\"đã\",\"nhiều\",\n",
        "            \"trên\",\"từ\",\"vào\",\"đó\",\"những\",\"ở\",\"ra\",\"tại\",\"lại\",\"cũng\",\"phải\",\"còn\"]\n",
        "\n",
        "def remove_stopwords(line):\n",
        "    words = []\n",
        "    for word in line.strip().split():\n",
        "        if word not in stopword:\n",
        "            words.append(word)\n",
        "    return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "rOIJJXGOu7fo"
      },
      "outputs": [],
      "source": [
        "def tien_xu_ly_van_ban(document):\n",
        "    # chuẩn hóa unicode\n",
        "    document = convert_unicode(document)\n",
        "    # chuẩn hóa cách gõ dấu tiếng Việt\n",
        "    document = chuan_hoa_dau_cau_tieng_viet(document)\n",
        "    # tách từ\n",
        "    document = word_tokenize(document, format=\"text\")\n",
        "    # đưa về lower\n",
        "    document = document.lower()\n",
        "    # xóa các ký tự không cần thiết\n",
        "    document = re.sub(r'[^\\s\\wáàảãạăắằẳẵặâấầẩẫậéèẻẽẹêếềểễệóòỏõọôốồổỗộơớờởỡợíìỉĩịúùủũụưứừửữựýỳỷỹỵđ_]',' ',document)\n",
        "    # xóa khoảng trắng thừa\n",
        "    document = re.sub(r'\\s+', ' ', document).strip()\n",
        "\n",
        "    document = remove_stopwords(document)\n",
        "    return document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "9QVT75WNcyX5"
      },
      "outputs": [],
      "source": [
        "# import OS module\n",
        "import os\n",
        "\n",
        "# Get the list of all files and directories\n",
        "the_loai = ['doi-song','du-lich','the-thao']\n",
        "labels = ['__label__đời_sống','__label__du_lịch','__label__thể_thao']\n",
        "\n",
        "path = \"./data/results/\"\n",
        "\n",
        "    \n",
        "with open('./data/results/data.txt', 'w', encoding='utf-8') as output_file:\n",
        "    for i in range(3):\n",
        "        list_files = os.listdir(path+the_loai[i])\n",
        "        # print(list_files)\n",
        "        for file in list_files:\n",
        "            f = open(path+the_loai[i]+'/'+file, encoding=\"utf8\")\n",
        "            content = f.read()\n",
        "            tien_xu_ly = tien_xu_ly_van_ban(content)\n",
        "            # print(tien_xu_ly)\n",
        "\n",
        "            output = labels[i] + ' ' + tien_xu_ly\n",
        "            # print(output)\n",
        "            output_file.write(output)\n",
        "            output_file.write('\\n')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4mXax2Vz_6P",
        "outputId": "3f9eba89-bd97-4920-b62f-449050e60919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "104\n",
            "113\n",
            "102\n"
          ]
        }
      ],
      "source": [
        "data = open(\"./data/results/data.txt\", encoding='utf-8')\n",
        "text = data.read().split()\n",
        "\n",
        "thethao = 0\n",
        "dulich = 0\n",
        "doisong=0\n",
        "\n",
        "for line in text:\n",
        "  if(line == \"__label__thể_thao\"):\n",
        "    thethao+=1\n",
        "  if(line == \"__label__du_lịch\"):\n",
        "    dulich+=1\n",
        "  if(line == \"__label__đời_sống\"):\n",
        "    doisong+=1\n",
        "\n",
        "print(thethao)\n",
        "print(dulich)\n",
        "print(doisong)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTI1aTzS4t7q"
      },
      "source": [
        "##Tách dữ liệu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_z0mO0Gz_9Y",
        "outputId": "17fb7456-ccd9-4451-8642-9b922fd8d5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['__label__du_lịch', '__label__thể_thao', '__label__đời_sống'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Chia tập train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# tỉ lệ tập train - test là 8 : 2\n",
        "test_percent = 0.2\n",
        "\n",
        "text = []\n",
        "label = []\n",
        "path = open(\"/content/drive/MyDrive/Project_Khaikhoang/VNExpressCrawler/data/results/data.txt\")\n",
        "\n",
        "for line in path:\n",
        "    words = line.strip().split()\n",
        "    label.append(words[0])\n",
        "    text.append(' '.join(words[1:]))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=test_percent, random_state=42)\n",
        "\n",
        "# Lưu train/test data\n",
        "# Giữ nguyên train/test để về sau so sánh các mô hình cho công bằng\n",
        "with open('/content/drive/MyDrive/Project_Khaikhoang/VNExpressCrawler/data/train-test/train.txt', 'w') as fp:\n",
        "    for x, y in zip(X_train, y_train):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "with open('/content/drive/MyDrive/Project_Khaikhoang/VNExpressCrawler/data/train-test/test.txt', 'w') as fp:\n",
        "    for x, y in zip(X_test, y_test):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "# encode label\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "print(list(label_encoder.classes_), '\\n')\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCpi8VzY-z4y"
      },
      "source": [
        "##Import thư viện"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4dfz-MMM-vGL"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import time\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'d:\\\\Learn_space\\\\KhaiKhoang'"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cựu chủ_tịch lđbđ_tây ban_nha bị cấm ba năm vì hôn_nữ cầu_thủ ngày 30 10 fifa án phạt cấm ông luis rubiales tham_gia tham_gia hoạt_động bóng_đá ngoài tây ban_nha mọi cấp_độ sau vụ cưỡng_hôn_nữ cầu_thủ jenni hermoso fifa thông_báo phạt rubiales do hành_vi ông sân_khấu lễ trao giải sau trận chung_kết world cup nữ 2023 hôm 20 8 vi_phạm điều 13 quy_tắc kỷ_luật fifa lđbđ thế_giới công_bố chi_tiết phán_quyết thẩm_phán ủy ban kỷ_luật lệnh cấm ba năm rubiales sẽ kéo_dài qua world cup nam 2026 tổ_chức mỹ canada mexico fifa biết cựu chủ_tịch rfef 10 ngày yêu_cầu kháng_cáo trước ủy ban phúc_thẩm fifa rubiales có_thể yêu_cầu fifa cung_cấp chi_tiết quyết_định kỷ_luật vòng 10 ngày sau đệ đơn kháng_cáo ông có_thể kháng_cáo lên tòa_trọng_tài thể_thao ồn_ào liên_quan tới tuyển nữ_tây ban_nha phát_sinh rubiales loạt hành_động gây tranh_cãi sau trận chung_kết world cup nữ 2023 rubiales hôn_môi hermoso sau rằng nhận sự đồng_thuận nhưng nữ cầu_thủ phủ_nhận lần cựu chủ_tịch rfef nắm lấy háng mừng chiến_thắng tây ban_nha trước anh cõng nữ cầu_thủ athenea del castillo vai lễ mừng sau trận ban_đầu rubiales từ_chối xin_lỗi quyết_tâm giữ ghế chủ_tịch nhưng chấp_nhận từ_chức ngày 10 9 ông từ_bỏ chức_vụ phó chủ_tịch uefa mang về thu_nhập 265 000 usd mỗi năm fifa sau tạm đình_chỉ rubiales 90 ngày điều_tra hermoso gửi đơn khiếu_nại lên văn_phòng công_tố quốc_gia lá đơn cô chuyển tới tòa tối_cao_tây ban_nha vì_thế rubiales vẫn đang bị điều_tra hình_sự tây ban_nha ông phủ_nhận mọi hành_vi sai_trái rfef ban_đầu ủng_hộ rubiales nhưng do sức_ép tổ_chức bổ_nhiệm chủ_tịch tạm quyền pedro_rocha sau tuyên_bố hành_động rubiales đáng xấu_hổ ngày 5 9 rfef cách_chức hlv jorge vilda đưa tây ban_nha chức vô_địch world_cup nữ đầu_tiên tới ngày 15 9 tòa_án tối_cao_tây ban_nha buộc cựu chủ_tịch rubiales giữ khoảng_cách hermoso ít_nhất 200 mét nhưng tòa bác_bỏ đề_nghị phía công_tố về việc phong_tỏa tài_sản rubiales yêu_cầu cựu chủ_tịch rfef trình_diện trước tòa 15 ngày_một lần tuyển_thủ nữ tây ban_nha tẩy_chay liên_đoàn sau sự_việc chỉ đồng_ý trở_lại thi_đấu tổ_chức xuống_nước chấp_thuận bắt_đầu cải_tổ theo yêu_sách họ hôm 20 9 hermoso trở_lại đội_tuyển đợt tập_trung cuối tháng 10 trận gặp italy ngày 27 10 thụy_sĩ ngày 31 10 rubiales cựu cầu_thủ trưởng_thành lò đào_tạo trẻ atletico từng cùng levante vô_địch giải hạng nhì tây ban_nha năm 2004 lên chơi la_liga sau giải_nghệ năm 2009 rubiales bầu làm chủ_tịch hiệp_hội cầu_thủ tây ban_nha rồi đắc_cử chủ_tịch rfef tháng 5 2018 quyết_định lớn đầu_tiên rubiales sa_thải hlv julen_lopetegui ngay trước tây ban_nha dự_world cup 2018 do hlv lén_lút thỏa_thuận dẫn_dắt real madrid duy_hồng theo ap\n",
            "1\n",
            "Naive Bayes, Accuracy = 95.3125\n"
          ]
        }
      ],
      "source": [
        "# Chia tập train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "text = []\n",
        "label = []\n",
        "path = open(\"./data/results/data.txt\", encoding=\"utf-8\")\n",
        "\n",
        "for line in path:\n",
        "    words = line.strip().split()\n",
        "    label.append(words[0])\n",
        "    text.append(' '.join(words[1:]))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=0.2, random_state=42)\n",
        "\n",
        "# Lưu train/test data\n",
        "# Giữ nguyên train/test để về sau so sánh các mô hình cho công bằng\n",
        "with open('./data/train-test/train.txt', 'w', encoding=\"utf-8\") as fp:\n",
        "    for x, y in zip(X_train, y_train):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "with open('./data/train-test/test.txt', 'w', encoding=\"utf-8\") as fp:\n",
        "    for x, y in zip(X_test, y_test):\n",
        "        fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "# encode label\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "# print(list(label_encoder.classes_), '\\n')\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "print(X_train[3])\n",
        "print(y_train[3])\n",
        "\n",
        "##Naive Bayes\n",
        "start_time = time.time()\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                              max_df=0.8,\n",
        "                                              max_features=None)),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', SVC(gamma='scale'))\n",
        "                      ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "# Save model\n",
        "pickle.dump(text_clf, open(os.path.join(\"./model\", \"svm.pkl\"), 'wb'))\n",
        "\n",
        "train_time = time.time() - start_time\n",
        "# print('Done training Naive Bayes in', train_time, 'seconds.')\n",
        "# Đánh giá mô hình Naive Bayes\n",
        "model = pickle.load(open(os.path.join(\"./model/\",\"svm.pkl\"), 'rb'))\n",
        "y_pred = model.predict(X_test)\n",
        "print('Naive Bayes, Accuracy =', accuracy_score(y_test,y_pred)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression, Accuracy:  93.75\n",
            "Logistic Regression, Accuracy:  96.88\n",
            "Logistic Regression, Accuracy:  95.31\n",
            "                 Model  Accuracy Score\n",
            "1          Naive Bayes           93.75\n",
            "2  Logistic Regression           96.88\n",
            "3                  SVM           95.31\n"
          ]
        }
      ],
      "source": [
        "# Chia tập train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "text = []\n",
        "label = []\n",
        "path = open(\"./data/results/data.txt\", encoding=\"utf-8\")\n",
        "\n",
        "# Tách nhãn và nội dung bài viết\n",
        "for line in path:\n",
        "    words = line.strip().split()\n",
        "    label.append(words[0])\n",
        "    text.append(' '.join(words[1:]))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=0.2, random_state=42)\n",
        "\n",
        "# encode label\n",
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(y_train)\n",
        "# print(list(label_encoder.classes_), '\\n')\n",
        "y_train = label_encoder.transform(y_train)\n",
        "y_test = label_encoder.transform(y_test)\n",
        "\n",
        "##Naive Bayes\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                              max_df=0.8,\n",
        "                                              max_features=None)),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', MultinomialNB())\n",
        "                      ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "# Đánh giá mô hình Naive Bayes\n",
        "model = text_clf\n",
        "y_pred = model.predict(X_test)\n",
        "naive = round(accuracy_score(y_test,y_pred)*100,2)\n",
        "print('Logistic Regression, Accuracy: ', naive)\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                              max_df=0.8,\n",
        "                                              max_features=None)),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', LogisticRegression(solver='lbfgs',\n",
        "                                                  multi_class='auto',\n",
        "                                                  max_iter=10000))\n",
        "                      ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "# Đánh giá mô hình Naive Bayes\n",
        "model = text_clf\n",
        "y_pred = model.predict(X_test)\n",
        "logistic = round(accuracy_score(y_test,y_pred)*100,2)\n",
        "print('Logistic Regression, Accuracy: ', logistic)\n",
        "text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                              max_df=0.8,\n",
        "                                              max_features=None)),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', SVC(gamma='scale'))\n",
        "                      ])\n",
        "text_clf = text_clf.fit(X_train, y_train)\n",
        "# Đánh giá mô hình Naive Bayes\n",
        "model = text_clf\n",
        "y_pred = model.predict(X_test)\n",
        "svm = round(accuracy_score(y_test,y_pred)*100,2)\n",
        "print('Logistic Regression, Accuracy: ', svm)\n",
        "\n",
        "import pandas as pd\n",
        "s = {'Model': pd.Series(['Naive Bayes','Logistic Regression', 'SVM'], index=['1', '2', '3']),\n",
        "     'Accuracy Score': pd.Series([naive, logistic, svm], index=['1', '2', '3'])}\n",
        "\n",
        "# tại DataFrame từ dict\n",
        "df = pd.DataFrame(s)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUw5STn19OFi"
      },
      "source": [
        "##Tổng hợp phân loại đánh giá mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJjJiV220AAH",
        "outputId": "dfe33d8b-f829-4e37-bf7d-2aec611eada7"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Project_Khaikhoang/VNExpressCrawler/data/results/data.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32md:\\Learn_space\\KhaiKhoang\\Crawling_data.ipynb Cell 29\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Learn_space/KhaiKhoang/Crawling_data.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m text \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Learn_space/KhaiKhoang/Crawling_data.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m label \u001b[39m=\u001b[39m []\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Learn_space/KhaiKhoang/Crawling_data.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m path \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m/content/drive/MyDrive/Project_Khaikhoang/VNExpressCrawler/data/results/data.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Learn_space/KhaiKhoang/Crawling_data.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m path:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Learn_space/KhaiKhoang/Crawling_data.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     words \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Project_Khaikhoang/VNExpressCrawler/data/results/data.txt'"
          ]
        }
      ],
      "source": [
        "i=5\n",
        "j=1\n",
        "avg_bayes = 0\n",
        "avg_logistic = 0\n",
        "avg_svm = 0\n",
        "\n",
        "for i in range(10):\n",
        "  # Chia tập train/test\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "  # tỉ lệ tập train - test là 8 : 2\n",
        "  test_percent = 0.2\n",
        "\n",
        "  text = []\n",
        "  label = []\n",
        "  path = open(\"/content/drive/MyDrive/Project_Khaikhoang/VNExpressCrawler/data/results/data.txt\")\n",
        "\n",
        "  for line in path:\n",
        "      words = line.strip().split()\n",
        "      label.append(words[0])\n",
        "      text.append(' '.join(words[1:]))\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=test_percent, random_state=i)\n",
        "\n",
        "  # Lưu train/test data\n",
        "  # Giữ nguyên train/test để về sau so sánh các mô hình cho công bằng\n",
        "  with open('/content/drive/MyDrive/Project_Khaikhoang/VNExpressCrawler/data/train-test/train.txt', 'w') as fp:\n",
        "      for x, y in zip(X_train, y_train):\n",
        "          fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "  with open('/content/drive/MyDrive/Project_Khaikhoang/VNExpressCrawler/data/train-test/test.txt', 'w') as fp:\n",
        "      for x, y in zip(X_test, y_test):\n",
        "          fp.write('{} {}\\n'.format(y, x))\n",
        "\n",
        "  # encode label\n",
        "  label_encoder = LabelEncoder()\n",
        "  label_encoder.fit(y_train)\n",
        "  # print(list(label_encoder.classes_), '\\n')\n",
        "  y_train = label_encoder.transform(y_train)\n",
        "  y_test = label_encoder.transform(y_test)\n",
        "  print(\"Lần lặp \",j)\n",
        "\n",
        "##Naive Bayes\n",
        "  start_time = time.time()\n",
        "  text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                              max_df=0.8,\n",
        "                                              max_features=None)),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', MultinomialNB())\n",
        "                      ])\n",
        "  text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "  # Save model\n",
        "  pickle.dump(text_clf, open(os.path.join(\"D:\\Learn_space\\KhaiKhoang\\model\", \"naive_bayes.pkl\"), 'wb'))\n",
        "  train_time = time.time() - start_time\n",
        "  # print('Done training Naive Bayes in', train_time, 'seconds.')\n",
        "  # Đánh giá mô hình Naive Bayes\n",
        "  model = pickle.load(open(os.path.join(\"D:\\Learn_space\\KhaiKhoang\\model\",\"naive_bayes.pkl\"), 'rb'))\n",
        "  y_pred = model.predict(X_test)\n",
        "  print('Naive Bayes, Accuracy =', accuracy_score(y_test,y_pred)*100)\n",
        "  avg_bayes+=accuracy_score(y_test,y_pred);\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ##Logistic Regression\n",
        "  start_time = time.time()\n",
        "  text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                              max_df=0.8,\n",
        "                                              max_features=None)),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', LogisticRegression(solver='lbfgs',\n",
        "                                                  multi_class='auto',\n",
        "                                                  max_iter=10000))\n",
        "                      ])\n",
        "  text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "  train_time = time.time() - start_time\n",
        "  # Save model\n",
        "  pickle.dump(text_clf, open(os.path.join(\"D:\\Learn_space\\KhaiKhoang\\model\", \"linear_classifier.pkl\"), 'wb'))\n",
        "  # print('Done training Linear Classifier in', train_time, 'seconds.')\n",
        "  #Đánh giá mô hình Linear Classifier\n",
        "  model = pickle.load(open(os.path.join(\"D:\\Learn_space\\KhaiKhoang\\model\",\"linear_classifier.pkl\"), 'rb'))\n",
        "  y_pred = model.predict(X_test)\n",
        "  print('Linear Classifier, Accuracy =', accuracy_score(y_test,y_pred)*100)\n",
        "  avg_logistic+=accuracy_score(y_test,y_pred);\n",
        "\n",
        "\n",
        "  #Phân loại văn bản SVM\n",
        "\n",
        "  start_time = time.time()\n",
        "  text_clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,1),\n",
        "                                              max_df=0.8,\n",
        "                                              max_features=None)),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', SVC(gamma='scale'))\n",
        "                      ])\n",
        "  text_clf = text_clf.fit(X_train, y_train)\n",
        "\n",
        "  train_time = time.time() - start_time\n",
        "  # print('Done training SVM in', train_time, 'seconds.')\n",
        "\n",
        "  # Save model\n",
        "  pickle.dump(text_clf, open(os.path.join(\"D:\\Learn_space\\KhaiKhoang\\model\", \"svm.pkl\"), 'wb'))\n",
        "  #Đánh giá mô hình SVM\n",
        "  model = pickle.load(open(os.path.join(\"D:\\Learn_space\\KhaiKhoang\\model\",\"svm.pkl\"), 'rb'))\n",
        "  y_pred = model.predict(X_test)\n",
        "  print('SVM, Accuracy =',accuracy_score(y_test,y_pred)*100)\n",
        "  avg_svm+=accuracy_score(y_test,y_pred);\n",
        "\n",
        "  j+=1\n",
        "  i=i*5\n",
        "  print(\"-----------------------------------\")\n",
        "\n",
        "\n",
        "print(\"Trung bình Độ chính xác sau 10 lần lặp\");\n",
        "print(\"Bayes:\", (avg_bayes/10)*100,\"%\")\n",
        "print(\"Logistics Regression:\",(avg_logistic/10)*100,\"%\")\n",
        "print(\"SVM:\",(avg_svm/10)*100,\"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGuQBpcBFXaR"
      },
      "source": [
        "##Đánh giá kết quả từng nhãn của mô hình naive bayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "w5ga3Opr8GL2",
        "outputId": "9c0a3ce9-8767-4479-c62d-17f429ff35c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   precision    recall  f1-score   support\n",
            "\n",
            " __label__du_lịch       0.87      0.87      0.87        23\n",
            "__label__thể_thao       1.00      0.89      0.94        28\n",
            "__label__đời_sống       0.75      0.92      0.83        13\n",
            "\n",
            "         accuracy                           0.89        64\n",
            "        macro avg       0.87      0.90      0.88        64\n",
            "     weighted avg       0.90      0.89      0.89        64\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Xem kết quả trên từng nhãn\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "nb_model = pickle.load(open(os.path.join(\"/content/drive/MyDrive/Project_Khaikhoang/VNExpressCrawler/models/\",\"naive_bayes.pkl\"), 'rb'))\n",
        "y_pred = nb_model.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=list(label_encoder.classes_)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2FIThJPS8GN6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDBmKZJVcy4G"
      },
      "source": [
        "##Test thêm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaH9oIs8gw-9",
        "outputId": "51d38921-9205-435f-809a-2aa007236b79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawl articles type giai-tri\n",
            "100% 1/1 [00:00<00:00,  1.24it/s]\n",
            "100% 47/47 [00:42<00:00,  1.11it/s]\n"
          ]
        }
      ],
      "source": [
        "!python types_crawler.py --type giai-tri --pages 1 --output /content/VNExpressCrawler/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X8JUywpgxUU",
        "outputId": "b2e49ba2-7ea1-40df-bf97-6c090aeacb42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawl articles type giao-duc\n",
            "100% 1/1 [00:00<00:00,  1.22it/s]\n",
            "100% 47/47 [00:44<00:00,  1.06it/s]\n"
          ]
        }
      ],
      "source": [
        "!python types_crawler.py --type giao-duc --pages 1 --output /content/VNExpressCrawler/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ-XT5XOgxaX",
        "outputId": "1184df03-c419-4408-feea-fac08cc0162b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crawl articles type khoa-hoc\n",
            "100% 1/1 [00:00<00:00,  1.28it/s]\n",
            "100% 35/35 [00:30<00:00,  1.13it/s]\n"
          ]
        }
      ],
      "source": [
        "!python types_crawler.py --type khoa-hoc --pages 1 --output /content/VNExpressCrawler/data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
